# API Configuration
  # This should be overridden by CONFIG.lms_api from .env with api key
lms_api_base_url: "http://some_free_VLM_api:1234/v1"
chat_endpoint: "/chat/completions"

# Model Configuration
model: "qwen/qwen2.5-vl-7b"
captioning_prompt_path: "models/vlm/captioning_prompt.txt"
question_prompt_path: "models/vlm/question_prompt.txt"

# Request Configuration
headers:
  Content-Type: "application/json"

# Generation Parameters
temperature: 0.4
top_k: 40
top_p: 0.95
min_p: 0.05
repeat_penalty: 1.1

# Response Processing
strip_json_markers: true # Whether to strip ```json and ``` from response